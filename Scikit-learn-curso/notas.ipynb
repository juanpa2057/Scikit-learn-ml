{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feactures = Caracteristigas\n",
    "Target = objetivo\n",
    "Las características, variables o atributos son las entradas medidas del dominio del problema, las variables independientes. La variable objetivo es la variable dependiente o la medida que intentamos modelar o pronosticar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features or variables or attributes are the measured inputs of the problem domain, the independent variables. The target variable is the dependent variable or the measure we're trying to model or forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curso  Profesional de Machine lerning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernesl : Es una funcíon matemática que toma mediciones que se comportan de manera no lineal y las proyecta en un espacio\n",
    "dimensional más grande donde sean linealmente separables. ej: si tenemos un punto en 2 dimensiones se aplica kernel para que esten 3 o 4 dimensiones se le sube la dimension a los datos. Sirven cuando los datos por medio de trazar una linea matematicamente no se pueden clasificar. Se aumenta la dimensionalidad a otro plano de tal manera q con una funcion lineal pudieramos lograr esa clasificación. \n",
    "\n",
    "- Algunos funciones kernel mas conocidas:  LINEALES - POLINOMIALES - GAUSSIANOS(RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regulazición: Consiste en disminuir la complejidad del modelo a través de  una penalización aplicada a sus variables mas irrelevantes.\n",
    "\n",
    "Pérdida o penalización: nos dice que tan lejos estamos de nuestros datos reales.\n",
    " Entre menor sea la pérdida mejor es el modelo.\n",
    "\n",
    "### 3 tipos de Regularización:\n",
    "- L1 LASSO: Reduccir la complejidad a través de la eliminación de feactures que no aportan demasiado al modelo.\n",
    "- L2 Ridge: Reducir la complejidad disminuyendo el impacto de ciertes feactures a nuestro modelo.\n",
    "- ElasticNet: Es una combinación de las dos anteriores.\n",
    "\n",
    "Un valor más bajo de la métrica de pérdida indica un mejor rendimiento del modelo.\n",
    "En este caso, el modelo con la menor pérdida es el modelo de regresión lineal\n",
    "- Linear Loss:  9.875049620340361e-08\n",
    "- Lasso loss:  0.05188912583862035\n",
    "- Ridge loss: 0.005502323291684915"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Que es un valor atípico?\n",
    "- Un valor atípico es cualquier medición que se encuentre por fuera del comportamiento general de una muestra de datos.\n",
    "- Pueden indicar variabilidad, errores de medición o novedades.\n",
    "\n",
    "### Porque son problematicos\n",
    "- Puedo generar sesgos importantes en los modelos de ML.\n",
    "- A veces contiene información relevante sobre la naturaleza de los datos.\n",
    "- Detección temprana a fallos.\n",
    "\n",
    "### Cómo identificarlos ? A través de métodos estadísticos:\n",
    "- 1. Z-SCORE: mide la distancia ( en desviaciones estándar) de un punto dado a la medida.\n",
    "- 2. Técmocas de clustering como DBSCAN\n",
    "- 3. Si q< Q1 -1.5 * IQR o q> Q3 +1.5*IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESIONES ROBUSTAS EN SCIKIT-LEARN\n",
    "- Nos ofrece algunos modelos específicos para abordar el problema de los valores atípicos:\n",
    "1. RANSAC: Ramdom sample consensus . Usamos una muestra aleatoria sobre el conjunto de datos q tenemos, buscamos la muestra que mas datos \"buenos\" logre incluir.\n",
    "2. Huber Reggresor: No ignora los valores atípicos, disminuye su influencia en el modelo. Los datos son tratados como atípicos si el erros absoluto de nuestra perdida está por encima de un umbral llamado epsilon.\n",
    "\n",
    "Se ha desmotrado que un valor de epsilon = 1.35 logra un 95% de eficiencia estadística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo de ensamble\n",
    "- Combinar diferentes métodos de ML con diferentes configuraciones y aplicar un método para lograr un consenso.\n",
    "- La diversidad es una muy buena opción.\n",
    "- Los métodos de ensamble se han destacado por ganar muchas competencias de ML.\n",
    "\n",
    "Funcionan muy bien cuando se prueban diferentes modelos o emsambladores para llegar a una unica respuestas.\n",
    "\n",
    "#### Dos estrategias: \n",
    "##### Bagging Bootstrap AGGgragation: \n",
    "- que tal si en un lugar de depender de la opion de un experto , consutalmos la opinion de varios expertos en pararelo.\n",
    "- Creamos particiones aleatorias(uniforme com reemplazo) del conjunto original.\n",
    "- Se construye modelos para cada uno de los set de los datos.\n",
    "- Cada uno de esos modelos nos das una respuesta y buscamos un metodo de consenso, entre todo este es el metodo de BAGGING\n",
    "\n",
    "#### modelos Ensamblados basados en BAGGING\n",
    "- Random Forest (RF)\n",
    "- Voting Classifiers/Regressors.\n",
    "- En general se puede aplicar cualquier modelos de la familia de modelos Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### el otro enfoque es Boosting , coger un modelo y fortalecer un modelo devil atraves de la sencuncial hasta que ya de un error menor\n",
    "#### Modelos ensamblasos basados en BOOSTING\n",
    "- AdaBoost.\n",
    "- Gradiente Tree Boosting.\n",
    "- XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategias de Clustering - Algortimo no supervisados (agrupacion de datos)\n",
    "\n",
    "los algoritmos de clustering son las estrategias que podemos usar para agrupar los datos de tal manera que todos los datos pertenecientes a un grupo sean los mas similares que sean posible entre sí, y lo más diferentes a los de otros grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El problema del clustering\n",
    "- no conocemos con anteioridad las etiquetas de nuestros datos( aprendizaje no supervisado).\n",
    "- Queremos descubrir patrones ocultos a simple vista.\n",
    "- Queremos identificar datos atípicos.\n",
    "\n",
    "Si conocemos el K, es decir cuantos grupos queremos clasificar usamos \n",
    "- K- MEANS\n",
    "- Spectral Clustering.\n",
    "\n",
    "Si no conoces el K, no sabes cuantas categorias usar:\n",
    "- Meanshift\n",
    "- Clustering jerarquico\n",
    "- DBScan\n",
    "\n",
    "Cuando son algoritmos de clasificacion ejemplo kmeans no se parte datas de prueba o testeo , si no que cogemnos todo el conjunto de datos para que los procese el algoritmo.\n",
    "\n",
    "#### Meanshift\n",
    "\n",
    "Cuando necesito que el algotimo decida cuantas categorias deben ir puedo usar meanshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación de modelos ( Cross Validation)\n",
    "1. La última palabra la tienen los datos.\n",
    "2. Necesitamos mentalidad de testeo.\n",
    "3. Todos los modelos son malos, solamente que algunos resultan útiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipos de validación.\n",
    "1. Dividir nuestros datos en Entrenamiento / Prueba (Hold-on)\n",
    "2. Usar Validación cruzada (K-Folds)\n",
    "3. Validacion Cruzada (LOOCV) # ESTE ES EL METODO MAS AGRASIVO\n",
    "\n",
    "Cuando usar HOLD-ON:\n",
    "1. Se requiere un proptotipado rápido.\n",
    "2. No se tiene mucho conocimiento.\n",
    "3. No se cuenta con abundante poder de cómputo.\n",
    "\n",
    "Cuando usar Validación K-FOLDS CV:\n",
    "1. Recomendable aen la mayoria en los casos.\n",
    "2. Se cuenta con un equipo suficiente para fesarrollar ML.\n",
    "3. Se requiere la integración con técnicas de optimización paramétrica.\n",
    "4. Se tiene más tiempo para las pruebas.\n",
    "\n",
    "Cuando usar LOOCV:\n",
    "1. Se tiene gran poder de cómputo.\n",
    "2. Se cuenta con pocos datos como para dividair por Training/test.\n",
    "3. Personas que sufren de TOC y quieren probar todos los casos posibles.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn tiene tres enfoques \n",
    "1. optimización manual.\n",
    "2. Optimización por grilla de parámetros. GridSearchCV.\n",
    "3. Optimización por búsqueda aleatorizada. RandomizedSearchCV.\n",
    "\n",
    "CON EL METODO RANDOMIZED podemos encontrar los parametros de una forma automatizada para nuestros modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La herramienta que te quiero presentar en esta clase se llama auto-sklearn, y nos ayudará a llevar aún un paso más lejos nuestro proceso de selección y optimización de modelos de machine learning. Dado que automáticamente prueba diferentes modelos predefinidos y configuraciones de parámetros comunes hasta encontrar la que más se ajuste según los datos que le pasemos como entrada. Con esta herramienta podrás entrenar modelos tanto de clasificación como de regresión por igual."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
